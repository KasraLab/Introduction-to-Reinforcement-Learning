{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comprehensive Guide to Reinforcement Learning\n",
    "\n",
    "Welcome to this comprehensive guide to Reinforcement Learning (RL). This notebook will walk you through the fundamental concepts, algorithms, and practical implementations of RL. We will cover everything from the basics of agents and environments to advanced topics like Deep Q-Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is a subfield of machine learning where an **agent** learns to make decisions by interacting with an **environment**. The agent's goal is to maximize the cumulative **reward** it receives over time.\n",
    "\n",
    "The core components of an RL system are:\n",
    "\n",
    "*   **Agent:** The learner or decision-maker.\n",
    "*   **Environment:** The external world with which the agent interacts.\n",
    "*   **State (s):** A representation of the environment's current situation.\n",
    "*   **Action (a):** A decision made by the agent.\n",
    "*   **Reward (r):** A scalar feedback signal from the environment that indicates how well the agent is doing.\n",
    "\n",
    "The agent and environment interact in a sequence of discrete time steps. At each time step *t*, the agent receives a state *S_t* and selects an action *A_t*. The environment responds with a new state *S_{t+1}* and a reward *R_{t+1}*. The agent's objective is to learn a **policy** (a mapping from states to actions) that maximizes the total expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Markov Decision Processes (MDPs)\n",
    "\n",
    "Most RL problems can be formalized as Markov Decision Processes (MDPs). An MDP is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker.\n",
    "\n",
    "An MDP is defined by a tuple `(S, A, P, R, \u03b3)`:\n",
    "\n",
    "*   **S:** A finite set of states.\n",
    "*   **A:** A finite set of actions.\n",
    "*   **P:** The state transition probability function, `P(s' | s, a)`, which is the probability of transitioning to state `s'` from state `s` after taking action `a`.\n",
    "*   **R:** The reward function, `R(s, a, s')`, which is the expected reward received after transitioning from state `s` to state `s'` by taking action `a`.\n",
    "*   **\u03b3 (gamma):** The discount factor, `0 <= \u03b3 <= 1`, which trades off the importance of immediate versus future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Bellman Equations\n",
    "\n",
    "The Bellman equations are a set of equations that decompose the value function into the immediate reward plus the discounted future values. They are fundamental to many RL algorithms.\n",
    "\n",
    "### State-Value Function (V(s))\n",
    "\n",
    "The state-value function `V(s)` is the expected return when starting in state `s` and following a policy `\u03c0` thereafter.\n",
    "\n",
    "The Bellman expectation equation for `V(s)` is:\n",
    "\n",
    "`V_\u03c0(s) = E_\u03c0[R_{t+1} + \u03b3V_\u03c0(S_{t+1}) | S_t = s]`\n",
    "\n",
    "### Action-Value Function (Q(s, a))\n",
    "\n",
    "The action-value function `Q(s, a)` is the expected return when starting in state `s`, taking action `a`, and then following a policy `\u03c0`.\n",
    "\n",
    "The Bellman expectation equation for `Q(s, a)` is:\n",
    "\n",
    "`Q_\u03c0(s, a) = E_\u03c0[R_{t+1} + \u03b3Q_\u03c0(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up the Environment\n",
    "\n",
    "Before we dive into the algorithms, let's install the necessary libraries. We will be using `gymnasium` for the environments and `numpy` for numerical computations. Later on, we will also use `stable-baselines3` for Deep RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A Simple GridWorld Environment\n",
    "\n",
    "To understand the RL algorithms, we will use a simple GridWorld environment. Our GridWorld is a 4x4 grid. The agent's goal is to navigate from a starting position to a goal position.\n",
    "\n",
    "Here are the details of our GridWorld:\n",
    "\n",
    "*   **State Space:** A 4x4 grid, so there are 16 states.\n",
    "*   **Action Space:** Four actions: Up, Down, Left, Right.\n",
    "*   **Rewards:**\n",
    "    *   +1 for reaching the goal.\n",
    "    *   -1 for falling into a hole.\n",
    "    *   0 for all other moves.\n",
    "*   **Dynamics:** The agent moves one step in the chosen direction. If the agent hits a wall, it stays in the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class GridWorldEnv:\n",
    "    def __init__(self, grid_size=4, start_state=0):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_states = grid_size * grid_size\n",
    "        self.n_actions = 4  # 0: Left, 1: Down, 2: Right, 3: Up\n",
    "        self.start_state = start_state\n",
    "        self.agent_state = start_state\n",
    "\n",
    "        self.goal_state = self.n_states - 1\n",
    "        self.holes = [5, 7, 11, 12]\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_state = self.start_state\n",
    "        return self.agent_state\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self._state_to_coords(self.agent_state)\n",
    "\n",
    "        if action == 0:  # Left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # Down\n",
    "            row = min(self.grid_size - 1, row + 1)\n",
    "        elif action == 2:  # Right\n",
    "            col = min(self.grid_size - 1, col + 1)\n",
    "        elif action == 3:  # Up\n",
    "            row = max(0, row - 1)\n",
    "\n",
    "        next_state = self._coords_to_state(row, col)\n",
    "        self.agent_state = next_state\n",
    "\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif next_state in self.holes:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _state_to_coords(self, state):\n",
    "        return state // self.grid_size, state % self.grid_size\n",
    "\n",
    "    def _coords_to_state(self, row, col):\n",
    "        return row * self.grid_size + col\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.grid_size, self.grid_size), '_', dtype=str)\n",
    "        for hole in self.holes:\n",
    "            row, col = self._state_to_coords(hole)\n",
    "            grid[row, col] = 'H'\n",
    "        goal_row, goal_col = self._state_to_coords(self.goal_state)\n",
    "        grid[goal_row, goal_col] = 'G'\n",
    "        agent_row, agent_col = self._state_to_coords(self.agent_state)\n",
    "        grid[agent_row, agent_col] = 'A'\n",
    "        print('\\n'.join([' '.join(row) for row in grid]))\n",
    "\n",
    "# Let's test the environment\n",
    "env = GridWorldEnv()\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"\\nTaking action: Right\")\n",
    "next_state, reward, done, _ = env.step(2) # 2 is Right\n",
    "env.render()\n",
    "print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model-Free Prediction\n",
    "\n",
    "Model-free methods learn directly from experience, without a model of the environment's dynamics. We will look at two model-free prediction methods: Monte Carlo and Temporal-Difference (TD) learning. These methods are used to estimate the value function of a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Monte Carlo (MC) Prediction\n",
    "\n",
    "Monte Carlo methods estimate the value function by averaging the returns observed after visiting a state. To use Monte Carlo methods, we need to have episodic tasks, meaning tasks that terminate.\n",
    "\n",
    "The algorithm for first-visit MC prediction is as follows:\n",
    "\n",
    "1.  Initialize `V(s)` and `Returns(s)` for all states `s`.\n",
    "2.  Loop forever (for each episode):\n",
    "    a. Generate an episode following the policy `\u03c0`.\n",
    "    b. For each state `s` appearing in the episode:\n",
    "        i.  `G` = return following the first occurrence of `s`.\n",
    "        ii. Append `G` to `Returns(s)`.\n",
    "        iii. `V(s)` = average of `Returns(s)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(env, policy, n_episodes, gamma=1.0):\n",
    "    V = np.zeros(env.n_states)\n",
    "    returns = {s: [] for s in range(env.n_states)}\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        visited_states = set()\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if state not in visited_states:\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "                visited_states.add(state)\n",
    "    return V\n",
    "\n",
    "# Let's define a simple policy: move right if possible, otherwise move down.\n",
    "policy = np.full(env.n_states, 2) # 2 is Right\n",
    "policy[3] = 1 # Down\n",
    "policy[7] = 1 # Down\n",
    "policy[15] = 0 # Goal state, action doesn't matter\n",
    "\n",
    "V_mc = mc_prediction(env, policy, n_episodes=1000)\n",
    "print(\"Value function estimated by Monte Carlo:\")\n",
    "print(V_mc.reshape(env.grid_size, env.grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Temporal-Difference (TD) Prediction\n",
    "\n",
    "Temporal-Difference (TD) learning is a combination of Monte Carlo and dynamic programming ideas. Like MC, TD methods learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for the final outcome (they bootstrap).\n",
    "\n",
    "The TD(0) update rule is:\n",
    "\n",
    "`V(S_t) <- V(S_t) + \u03b1[R_{t+1} + \u03b3V(S_{t+1}) - V(S_t)]`\n",
    "\n",
    "where `\u03b1` is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(env, policy, n_episodes, alpha=0.1, gamma=1.0):\n",
    "    V = np.zeros(env.n_states)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td_target = reward + gamma * V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] += alpha * td_error\n",
    "            state = next_state\n",
    "    return V\n",
    "\n",
    "V_td = td_prediction(env, policy, n_episodes=1000)\n",
    "print(\"Value function estimated by TD(0):\")\n",
    "print(V_td.reshape(env.grid_size, env.grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model-Free Control\n",
    "\n",
    "Control is about finding the optimal policy. In the model-free setting, we do not know the environment's dynamics. We will explore two popular TD control algorithms: Q-Learning (off-policy) and SARSA (on-policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Q-Learning (Off-Policy TD Control)\n",
    "\n",
    "Q-Learning is an off-policy TD control algorithm. Off-policy means that the policy being learned about (the greedy policy) is different from the policy used to generate the data (the epsilon-greedy policy).\n",
    "\n",
    "The Q-Learning update rule is:\n",
    "\n",
    "`Q(S_t, A_t) <- Q(S_t, A_t) + \u03b1[R_{t+1} + \u03b3 * max_a Q(S_{t+1}, a) - Q(S_t, A_t)]`\n",
    "\n",
    "We use an epsilon-greedy policy for action selection to ensure exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((env.n_states, env.n_actions))\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(env.n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "Q_q_learning = q_learning(env, n_episodes=10000)\n",
    "policy_q = np.argmax(Q_q_learning, axis=1)\n",
    "\n",
    "print(\"Optimal policy from Q-Learning:\")\n",
    "print(policy_q.reshape(env.grid_size, env.grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. SARSA (On-Policy TD Control)\n",
    "\n",
    "SARSA is an on-policy TD control algorithm. On-policy means that the policy being learned about is the same as the policy used to generate the data. The name SARSA comes from the sequence of events: State, Action, Reward, State, Action.\n",
    "\n",
    "The SARSA update rule is:\n",
    "\n",
    "`Q(S_t, A_t) <- Q(S_t, A_t) + \u03b1[R_{t+1} + \u03b3 * Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]`\n",
    "\n",
    "Notice the difference from Q-Learning: we use the Q-value of the action that was actually taken in the next state, not the maximum Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, n_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((env.n_states, env.n_actions))\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(env.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action = np.random.randint(env.n_actions)\n",
    "            else:\n",
    "                next_action = np.argmax(Q[next_state])\n",
    "\n",
    "            td_target = reward + gamma * Q[next_state, next_action]\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    return Q\n",
    "\n",
    "Q_sarsa = sarsa(env, n_episodes=10000)\n",
    "policy_sarsa = np.argmax(Q_sarsa, axis=1)\n",
    "\n",
    "print(\"Optimal policy from SARSA:\")\n",
    "print(policy_sarsa.reshape(env.grid_size, env.grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Reinforcement Learning\n",
    "\n",
    "For problems with large state spaces, using a table to store Q-values is not feasible. Deep Reinforcement Learning (DRL) uses deep neural networks to approximate the value function or the policy. This allows us to solve much more complex problems.\n",
    "\n",
    "### 8.1. Deep Q-Networks (DQN)\n",
    "\n",
    "A Deep Q-Network (DQN) is a neural network that approximates the action-value function `Q(s, a)`. The network takes the state `s` as input and outputs the Q-values for all possible actions.\n",
    "\n",
    "Two key innovations in DQN are:\n",
    "\n",
    "1.  **Experience Replay:** A buffer of past experiences `(s, a, r, s')` is stored. The network is trained on random mini-batches from this buffer, which breaks the correlation between consecutive samples.\n",
    "2.  **Target Network:** A separate, fixed target network is used to generate the TD target. This helps to stabilize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra] gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. DQN with Stable Baselines3\n",
    "\n",
    "We will use the `stable-baselines3` library to train a DQN agent on the `LunarLander-v2` environment. `stable-baselines3` is a set of reliable implementations of reinforcement learning algorithms in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instantiate the DQN model\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Evaluate the trained model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Problems and Solutions\n",
    "\n",
    "This section contains a few exercises to help you solidify your understanding of the concepts covered in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Modify the GridWorld Environment\n",
    "\n",
    "**Task:** Modify the `GridWorldEnv` to include a new element: a 'windy' condition. If the agent is in a certain column, there's a 50% chance it will be pushed one step up. \n",
    "\n",
    "**Hint:** Modify the `step` function. Add a condition to check if the agent is in a windy column and apply the wind effect with a certain probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridWorldEnv(GridWorldEnv):\n",
    "    def __init__(self, grid_size=4, start_state=0, windy_cols=[1, 2]):\n",
    "        super().__init__(grid_size, start_state)\n",
    "        self.windy_cols = windy_cols\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self._state_to_coords(self.agent_state)\n",
    "        \n",
    "        # Apply wind\n",
    "        if col in self.windy_cols and np.random.rand() < 0.5:\n",
    "            row = max(0, row - 1)\n",
    "\n",
    "        if action == 0:  # Left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 1:  # Down\n",
    "            row = min(self.grid_size - 1, row + 1)\n",
    "        elif action == 2:  # Right\n",
    "            col = min(self.grid_size - 1, col + 1)\n",
    "        elif action == 3:  # Up\n",
    "            row = max(0, row - 1)\n",
    "\n",
    "        next_state = self._coords_to_state(row, col)\n",
    "        self.agent_state = next_state\n",
    "\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif next_state in self.holes:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Test the windy environment\n",
    "windy_env = WindyGridWorldEnv()\n",
    "state = windy_env.reset()\n",
    "windy_env.render()\n",
    "print(\"\\nTaking action in a windy column...\")\n",
    "windy_env.agent_state = 6 # (row=1, col=2), a windy column\n",
    "next_state, reward, done, _ = windy_env.step(1) # Down\n",
    "windy_env.render()\n",
    "print(f\"Next State: {next_state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Implement Decaying Epsilon for Q-Learning\n",
    "\n",
    "**Task:** Modify the `q_learning` function to include a decaying epsilon. The epsilon value should decrease over time, so the agent explores less as it learns more. A common strategy is to use exponential decay.\n",
    "\n",
    "**Hint:** Add `epsilon_decay`, `min_epsilon` parameters. In each episode, update epsilon: `epsilon = max(min_epsilon, epsilon * epsilon_decay)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_decaying_epsilon(env, n_episodes, alpha=0.1, gamma=0.99, epsilon=1.0, min_epsilon=0.01, epsilon_decay=0.995):\n",
    "    Q = np.zeros((env.n_states, env.n_actions))\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(env.n_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state, best_next_action]\n",
    "            td_error = td_target - Q[state, action]\n",
    "            Q[state, action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    return Q\n",
    "\n",
    "Q_q_learning_decay = q_learning_decaying_epsilon(env, n_episodes=10000)\n",
    "policy_q_decay = np.argmax(Q_q_learning_decay, axis=1)\n",
    "\n",
    "print(\"Optimal policy from Q-Learning with decaying epsilon:\")\n",
    "print(policy_q_decay.reshape(env.grid_size, env.grid_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
